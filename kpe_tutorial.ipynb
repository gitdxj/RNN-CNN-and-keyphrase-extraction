{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kpe_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KThqd8B30h4O"
      },
      "source": [
        "# Keyphrase Extraction with `pke`\r\n",
        "\r\n",
        "[`pke`](https://github.com/boudinfl/pke) (Python Keyphrase Extraction) by Boudin et al. is probably the most comprehensive collection of algorithms related to keyphrase extraction.\r\n",
        "It implements both a variety of supervised and unsupervised models, although they are primarily focused around traditional approaches, and not neural networks.  \r\n",
        "However, it offers a unified interface to the various approaches, which is a huge plus over related libraries.\r\n",
        "\r\n",
        "Advantages:\r\n",
        "* Easy-to-use API\r\n",
        "* Several different models available\r\n",
        "* Somewhat regular updates\r\n",
        "\r\n",
        "Disadvantages:\r\n",
        "* No neural models implemented (yet)\r\n",
        "* GPL license (restrictive for commercial use ;-) )\r\n",
        "\r\n",
        "Available on Github: https://github.com/boudinfl/pke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22jxjlNx1RZH"
      },
      "source": [
        "## Existing Datasets and Other Libraries\r\n",
        "\r\n",
        "(Campos et al.) have a neat collection of KPE datasets, mostly focusing on scientific articles (but in a variety of languages!  \r\n",
        "https://github.com/LIAAD/KeywordExtractor-Datasets\r\n",
        "\r\n",
        "The KP20k dataset from (Meng et al., 2017) can also be found online, including the entire 570,000 documents used for training, validation *and* testing. See: https://github.com/memray/OpenNMT-kpg-release  \r\n",
        "Note that this is associated with a follow-up paper not covered in the lecture. The original repository for the \"Deep Keyphrase Generation\" paper is [here](https://github.com/memray/seq2seq-keyphrase). Although I cannot really recommend the model for anything beyond the actual architecture, because it is unfortunately implemented in theano.\r\n",
        "\r\n",
        "For SotA KPE prediction, the team behind Span-BERT  (Sun et al., 2020) has [released their code](https://github.com/thunlp/BERT-KPE) as well. However, I have not checked how easy it is to use (or set up with custom data samples).\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M_w0lSjHU1s"
      },
      "source": [
        "## Careful with Other Implementations\r\n",
        "I generally advise you to be careful about choosing other libraries. Especially for popular methods (such as RAKE or TextRank), there are quite a few different implementations. At least for some, there are implementation errors in the respective code repositories, which leads to worse/wrong results! If you are looking for a correct implementation for RAKE, the implementation by Alyona Medelyan (https://github.com/zelandiya/RAKE-tutorial) is probably your best shot.\r\n",
        "\r\n",
        "`pke` at least has somewhat decent testing, which should provide enough resilience against completely faulty implementations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHkTbHaFGMro",
        "outputId": "37c1040c-525d-4e97-e903-0655d24d0f94"
      },
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-99lyrwdw\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-99lyrwdw\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (3.2.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (2.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (0.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pke==1.8.1) (4.4.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (0.8.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (51.3.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->pke==1.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.7.4.3)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-1.8.1-cp36-none-any.whl size=8763600 sha256=a9e9e1738d0343a3de835a01e8dbdc2c9a6a4701aed4a14f2ae47d4b6301958c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sp03hdk4/wheels/8d/24/54/6582e854e9e32dd6c632af6762b3a5d2f6b181c2992e165462\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-1.8.1 unidecode-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wymfUfIMGP94",
        "outputId": "abdb76b0-35f7-4091-9715-aa7b4e36cda6"
      },
      "source": [
        "!python -m nltk.downloader stopwords\r\n",
        "!python -m nltk.downloader universal_tagset\r\n",
        "!python -m spacy download en # download the english model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDSnWl9mHK3o"
      },
      "source": [
        "## Input Formats\r\n",
        "For reference, see here: https://boudinfl.github.io/pke/build/html/tutorials/input.html\r\n",
        "\r\n",
        "Mostly, you will want to load directly from a file or text that you already have in memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_UvlBCtGZkl",
        "outputId": "b8aedf54-d2c0-4ca5-e032-50e034038c98"
      },
      "source": [
        "import pke\r\n",
        "from urllib.request import urlopen\r\n",
        "\r\n",
        "# Sherlock Holmes book\r\n",
        "data = urlopen(\"https://sherlock-holm.es/stories/plain-text/advs.txt\")\r\n",
        "\r\n",
        "sample_text = \"\"\r\n",
        "for line in data:\r\n",
        "  sample_text += str(line)\r\n",
        "\r\n",
        "print(sample_text[0:500])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\n'b'\\n'b'\\n'b'\\n'b'                        THE ADVENTURES OF SHERLOCK HOLMES\\n'b'\\n'b'                               Arthur Conan Doyle\\n'b'\\n'b'\\n'b'\\n'b'                                Table of contents\\n'b'\\n'b'               A Scandal in Bohemia\\n'b'               The Red-Headed League\\n'b'               A Case of Identity\\n'b'               The Boscombe Valley Mystery\\n'b'               The Five Orange Pips\\n'b'               The Man with the Twisted Lip\\n'b'               The Adventure \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grd_pcmLJanz"
      },
      "source": [
        "extractor = pke.unsupervised.YAKE\r\n",
        "\r\n",
        "extractor.load_document(input=sample_text, language=\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr-1SrZcKhjN",
        "outputId": "9c36efda-3b98-49be-c7d7-67b32b334151"
      },
      "source": [
        "len(extractor.sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-q5SU85KnXI"
      },
      "source": [
        "# Alternatively, load from a file.\r\n",
        "with open(\"./sample_text.txt\", \"w\") as f:\r\n",
        "  f.write(sample_text)\r\n",
        "\r\n",
        "extractor2 = pke.unsupervised.YAKE()\r\n",
        "extractor2.load_document(input=\"./sample_text.txt\", language=\"en\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InZP64BRLHSP",
        "outputId": "e17ef8a5-5d47-4500-91f9-d62a3a86e12c"
      },
      "source": [
        "len(extractor2.sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_dwQEGKLsZJ"
      },
      "source": [
        "## Hints about Non-English Input Formats\r\n",
        "`pke` works with inputs both in ASCII and unicode. This means you can theoretically load documents in other languages, however, this would require you to provide custom stopword lists and other language modules. Since `pke` uses spaCy as an underlying module, the language is working as long as you have the correct spaCy language model installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4HsQSjMLVCv"
      },
      "source": [
        "## Selecting Algorithm Parameters\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYbjnG9OLX0K"
      },
      "source": [
        "# See https://boudinfl.github.io/pke/build/html/unsupervised.html#yake for description of YAKE's parameters\r\n",
        "\r\n",
        "# Select criteria for candidate phrase generation. Also allows for custom stopword lists.\r\n",
        "# n i n this case is referring to the maximum n-gram length of the candidate phrases\r\n",
        "extractor.candidate_selection(n=3)\r\n",
        "\r\n",
        "# Choose algorithm parameters\r\n",
        "extractor.candidate_weighting(window=2, use_stems=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTaGqMDuOgqe"
      },
      "source": [
        "## Select Keyphrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is50UW3AOlRZ",
        "outputId": "4ec746d8-cd4d-450d-e880-7cf202f90dc6"
      },
      "source": [
        "# Select how many keywords should be returned. Can be thresholded to only return \"important enough\" keyphrases\r\n",
        "keyphrases = extractor.get_n_best(15, threshold=0.5)\r\n",
        "\r\n",
        "print(keyphrases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('sherlock holm', 4.53937378536963e-05), ('said holm', 6.29030995107384e-05), ('holm', 8.156935573010463e-05), ('upon the tabl', 0.00021329219739364078), ('holm wa well', 0.0003197319806372044), ('said', 0.0003740219201981197), ('holm in baker', 0.0003844077980404051), ('upon', 0.0004117301383089954), ('littl of holm', 0.0004964470017736267), ('one', 0.0005117275249004911), ('well', 0.0006783442050531117), ('would', 0.000719145196883692), ('word to holm', 0.0008822938565316763), ('man', 0.0009060266248705548), ('holm the relentless', 0.0009793297745161844)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHMGpEy7OxJi"
      },
      "source": [
        "## Train a Supervised KEA Model\r\n",
        "So far, we have looked at a simple statistical model that does not rely on any further information besides the extraction parameters. However, for supervised models, we can also provide our self-trained models, or use the provided pre-trained models.\r\n",
        "\r\n",
        "For further information on training, see https://boudinfl.github.io/pke/build/html/tutorials/training.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk_yVVzRPI_V"
      },
      "source": [
        "# Compute document frequency statistics\r\n",
        "# Training files can be found here: https://sherlock-holm.es/ascii/\r\n",
        "pke.utils.compute_document_frequency(input_dir=\"./training/\", \r\n",
        "                                     output_file=\"./df_counts.gz\", \r\n",
        "                                     extension='txt', \r\n",
        "                                     language='en', \r\n",
        "                                     normalization='stemming', \r\n",
        "                                     stoplist=None, \r\n",
        "                                     delimiter='\\t', \r\n",
        "                                     n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba0OCjNOS6n3"
      },
      "source": [
        "# load the DF counts from file\r\n",
        "df_counts = pke.load_document_frequency_file(input_file='./df_counts.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCKed3PTvYx"
      },
      "source": [
        "# train a new Kea model\r\n",
        "pke.train_supervised_model(input_dir='/path/to/collection/of/documents/',\r\n",
        "                           reference_file='/path/to/reference/file',\r\n",
        "                           model_file='./trained_KEA.pke',\r\n",
        "                           df=df_counts,\r\n",
        "                           extension='txt',\r\n",
        "                           language='en',\r\n",
        "                           normalization=\"stemming\",\r\n",
        "                           model=pke.supervised.Kea())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}